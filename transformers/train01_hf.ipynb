{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4997ee2-aad2-437f-b37f-744be58f5d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "!echo $HF_ENDPOINT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77654b1d-f5be-4465-9874-36d058d40dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4caa284-232c-4567-bc45-48a51841b448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': '正常', 'score': 0.9747933149337769}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 仅指定任务时，使用默认模型（不推荐）\n",
    "pipe = pipeline(\"sentiment-analysis\", model=\"alibaba-pai/pai-bert-base-zh-llm-risk-detection\", tokenizer=\"alibaba-pai/pai-bert-base-zh-llm-risk-detection\", max_length=128)\n",
    "pipe(\"今儿上海可真冷啊\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5dfbee2-9058-4e57-98f1-d71fbc6e6996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '正常', 'score': 0.8297399878501892}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = pipe(\"我觉得这家店蒜泥白肉的味道一般\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b48a577b-7edf-4fa7-96ec-993f8a644ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '正常', 'score': 0.9713119268417358}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 默认使用的模型 distilbert-base-uncased-finetuned-sst-2-english \n",
    "# 并未针对中文做太多训练，中文的文本分类任务表现未必满意\n",
    "pipe(\"你学东西真的好快，理论课一讲就明白了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dae8d88e-5973-4078-ab42-e7bfbc6882c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/data/minconda3/envs/llm/lib/python3.11/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'ORG',\n",
       "  'score': 0.9674639,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 0,\n",
       "  'end': 12},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.99828726,\n",
       "  'word': 'French',\n",
       "  'start': 18,\n",
       "  'end': 24},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99896103,\n",
       "  'word': 'New York City',\n",
       "  'start': 42,\n",
       "  'end': 55}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(task=\"ner\", grouped_entities=True)\n",
    "classifier(\"Hugging Face is a French company based in New York City.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2374f316-833a-405e-8478-af244746a7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"couldn't find it in the cached files and it looks like t5-base is not the path to a directory containing a file named config.json.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "502a772a-0338-4318-9408-650d781d69a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /data/minconda3/envs/llm/lib/python3.11/site-packages/huggingface_hub-0.22.0rc0-py3.8.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting tokenizers_pegasus\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/31/0c/9e20fb8d456f9ed8b6789d734c01fedea9a5a9ddead83fbfabe8f83b5920/tokenizers_pegasus-0.0.8-py3-none-any.whl (12 kB)\n",
      "Collecting jieba (from tokenizers_pegasus)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /data/minconda3/envs/llm/lib/python3.11/site-packages (from tokenizers_pegasus) (1.26.4)\n",
      "Requirement already satisfied: rouge in /data/minconda3/envs/llm/lib/python3.11/site-packages (from tokenizers_pegasus) (1.0.1)\n",
      "Requirement already satisfied: six in /data/minconda3/envs/llm/lib/python3.11/site-packages (from tokenizers_pegasus) (1.16.0)\n",
      "Building wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=024b73a5cffc4144baf2b376cd40e0e2588cf0addc32d217606328aac9c2a34f\n",
      "  Stored in directory: /home/yzsmjh/.cache/pip/wheels/32/98/ba/a37fcadb96c75c8f9366a3d17da29cdf8a745ffd38d0092e0d\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba, tokenizers_pegasus\n",
      "Successfully installed jieba-0.42.1 tokenizers_pegasus-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers_pegasus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c444728-0d39-49fb-b260-4a44dadce7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 22:53:58.069213: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-25 22:53:58.108884: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-25 22:53:58.849396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03509676eb5943f7895afd377e085e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a505a1e1315342e2837331dfa254800b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9d9bcc1a21491982396df7dfa0079d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc58f61b6504622898772a73b8961a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1d0777e9e94fb5bcccc873e00df074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/minconda3/envs/llm/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(task=\"summarization\",\n",
    "                      model=\"t5-base\",\n",
    "                      min_length=8,\n",
    "                      max_length=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a561c5d2-e914-4b4e-b95b-e400dcb0b874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'the Transformer is the first sequence transduction model based entirely on attention . it replaces recurrent layers commonly used in encoder-decode'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(\n",
    "    \"\"\"\n",
    "    In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, \n",
    "    replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. \n",
    "    For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. \n",
    "    On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. \n",
    "    In the former task our best model outperforms even all previously reported ensembles.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9095af7-446a-4edf-9f87-654b532c7a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'large language models (LLMs) are very large deep learning models pre-trained on vast amounts of data . transformers are capable of un'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(\n",
    "    '''\n",
    "    Large language models (LLM) are very large deep learning models that are pre-trained on vast amounts of data. \n",
    "    The underlying transformer is a set of neural networks that consist of an encoder and a decoder with self-attention capabilities. \n",
    "    The encoder and decoder extract meanings from a sequence of text and understand the relationships between words and phrases in it.\n",
    "    Transformer LLMs are capable of unsupervised training, although a more precise explanation is that transformers perform self-learning. \n",
    "    It is through this process that transformers learn to understand basic grammar, languages, and knowledge.\n",
    "    Unlike earlier recurrent neural networks (RNN) that sequentially process inputs, transformers process entire sequences in parallel. \n",
    "    This allows the data scientists to use GPUs for training transformer-based LLMs, significantly reducing the training time.\n",
    "    '''\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f1e323-b919-4d4c-ae33-d2dd8b40e1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e959a323834b442dab4544422c52c39a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ac4ab0ea0c484499e342b7660dbd6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446f217c80d44fafae5d5d5a6b7dcbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/66.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "not a string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summarizer2 \u001b[38;5;241m=\u001b[39m pipeline(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarization\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m                       model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIDEA-CCNL/Randeng-Pegasus-523M-Chinese\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m                       min_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m      4\u001b[0m                       max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m      5\u001b[0m )\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/transformers/pipelines/__init__.py:967\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    964\u001b[0m             tokenizer_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    965\u001b[0m             tokenizer_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 967\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    968\u001b[0m             tokenizer_identifier, use_fast\u001b[38;5;241m=\u001b[39muse_fast, _from_pipeline\u001b[38;5;241m=\u001b[39mtask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs\n\u001b[1;32m    969\u001b[0m         )\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_image_processor:\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;66;03m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:832\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    830\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_fast\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2029\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2026\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2027\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[1;32m   2030\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   2031\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   2032\u001b[0m     init_configuration,\n\u001b[1;32m   2033\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[1;32m   2034\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   2035\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   2036\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   2037\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m   2038\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[1;32m   2039\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2040\u001b[0m )\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2061\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2059\u001b[0m has_tokenizer_file \u001b[38;5;241m=\u001b[39m resolved_vocab_files\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (from_slow \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_tokenizer_file) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2061\u001b[0m     slow_tokenizer \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class)\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[1;32m   2062\u001b[0m         copy\u001b[38;5;241m.\u001b[39mdeepcopy(resolved_vocab_files),\n\u001b[1;32m   2063\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   2064\u001b[0m         copy\u001b[38;5;241m.\u001b[39mdeepcopy(init_configuration),\n\u001b[1;32m   2065\u001b[0m         \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[1;32m   2066\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   2067\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   2068\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   2069\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39m_commit_hash,\n\u001b[1;32m   2070\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)),\n\u001b[1;32m   2071\u001b[0m     )\n\u001b[1;32m   2072\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2073\u001b[0m     slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2261\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2259\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2261\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[1;32m   2262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2266\u001b[0m     )\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/transformers/models/pegasus/tokenization_pegasus.py:150\u001b[0m, in \u001b[0;36mPegasusTokenizer.__init__\u001b[0;34m(self, vocab_file, pad_token, eos_token, unk_token, mask_token, mask_token_sent, additional_special_tokens, offset, sp_model_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_model \u001b[38;5;241m=\u001b[39m spm\u001b[38;5;241m.\u001b[39mSentencePieceProcessor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_model_kwargs)\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_model\u001b[38;5;241m.\u001b[39mLoad(vocab_file)\n\u001b[1;32m    152\u001b[0m _added_tokens_decoder \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;241m0\u001b[39m: AddedToken(\u001b[38;5;28mstr\u001b[39m(pad_token), special\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;241m1\u001b[39m: AddedToken(\u001b[38;5;28mstr\u001b[39m(eos_token), special\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    155\u001b[0m }\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_token_sent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/sentencepiece/__init__.py:961\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Load\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_proto:\n\u001b[1;32m    960\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadFromSerializedProto(model_proto)\n\u001b[0;32m--> 961\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadFromFile(model_file)\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/sentencepiece/__init__.py:316\u001b[0m, in \u001b[0;36mSentencePieceProcessor.LoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLoadFromFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _sentencepiece\u001b[38;5;241m.\u001b[39mSentencePieceProcessor_LoadFromFile(\u001b[38;5;28mself\u001b[39m, arg)\n",
      "\u001b[0;31mTypeError\u001b[0m: not a string"
     ]
    }
   ],
   "source": [
    "\n",
    "summarizer2 = pipeline(task=\"summarization\",\n",
    "                      model=\"IDEA-CCNL/Randeng-Pegasus-523M-Chinese\",\n",
    "                      min_length=8,\n",
    "                      max_length=32,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e847c2-5176-4566-8632-fb82fd31a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"据微信公众号“界面”报道，4日上午10点左右，中国发改委反垄断调查小组突击查访奔驰上海办事处，调取数据材料，并对多名奔驰高管进行了约谈。截止昨日晚9点，包括北京梅赛德斯-奔驰销售服务有限公司东区总经理在内的多名管理人员仍留在上海办公室内\"\n",
    "\n",
    "summarizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62fafa1e-6654-4d02-adea-4176092b91f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Generate Summary\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m summary_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     18\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(summary_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:1370\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1363\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1364\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration results, please set `padding_side=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` when initializing the tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1365\u001b[0m         )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1368\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;66;03m# and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1370\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_encoder_decoder_kwargs_for_generation(\n\u001b[1;32m   1371\u001b[0m         inputs_tensor, model_kwargs, model_input_name\n\u001b[1;32m   1372\u001b[0m     )\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/transformers/generation/utils.py:491\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    489\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    490\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 491\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m encoder(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoder_kwargs)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/transformers/models/pegasus/modeling_pegasus.py:746\u001b[0m, in \u001b[0;36mPegasusEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 746\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens(input_ids) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_scale\n\u001b[1;32m    748\u001b[0m embed_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_positions(input_shape)\n\u001b[1;32m    750\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m embed_pos\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/torch/nn/functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration\n",
    "# Need to download tokenizers_pegasus.py and other Python script from Fengshenbang-LM github repo in advance,\n",
    "# or you can download tokenizers_pegasus.py and data_utils.py in https://huggingface.co/IDEA-CCNL/Randeng_Pegasus_523M/tree/main\n",
    "# Strongly recommend you git clone the Fengshenbang-LM repo:\n",
    "# 1. git clone https://github.com/IDEA-CCNL/Fengshenbang-LM\n",
    "# 2. cd Fengshenbang-LM/fengshen/examples/pegasus/\n",
    "# and then you will see the tokenizers_pegasus.py and data_utils.py which are needed by pegasus model\n",
    "from tokenizers_pegasus import PegasusTokenizer\n",
    "\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-523M-Chinese\")\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"IDEA-CCNL/Randeng-Pegasus-523M-Chinese\")\n",
    "\n",
    "text = \"据微信公众号“界面”报道，4日上午10点左右，中国发改委反垄断调查小组突击查访奔驰上海办事处，调取数据材料，并对多名奔驰高管进行了约谈。截止昨日晚9点，包括北京梅赛德斯-奔驰销售服务有限公司东区总经理在内的多名管理人员仍留在上海办公室内\"\n",
    "inputs = tokenizer(text, max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"])\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "# model Output: 截止昨日晚9点，包括北京梅赛德斯-奔驰销售服务有限公司东区总经理在内的多名管理人员仍留在上海办公室内\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c861147-5873-4bbe-815b-28b201074170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at superb/hubert-base-superb-er were not used when initializing HubertForSequenceClassification: ['hubert.encoder.pos_conv_embed.conv.weight_g', 'hubert.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at superb/hubert-base-superb-er and are newly initialized: ['hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'hubert.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(task=\"audio-classification\", model=\"superb/hubert-base-superb-er\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f818fa61-c87a-4885-a365-5235f49c75aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/LLM-quickstart-main/transformers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    " \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63a8c92b-1015-4c6e-ab25-7036e2161bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(\"/data/LLM-quickstart-main/transformers/mlk.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d558265c-3f0d-443e-86a6-86d2e60953a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/Narsil/asr_dummy/resolve/main/mlk.flac (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x755f689aa610>: Failed to establish a new connection: [Errno 101] Network is unreachable'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/urllib3/connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    204\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m    206\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[1;32m    207\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[1;32m    208\u001b[0m     )\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 101] Network is unreachable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/urllib3/connectionpool.py:491\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    490\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/urllib3/connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/urllib3/connectionpool.py:1096\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m-> 1096\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/urllib3/connection.py:611\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    610\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[0;32m--> 611\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[1;32m    612\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/urllib3/connection.py:218\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x755f689aa610>: Failed to establish a new connection: [Errno 101] Network is unreachable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/urllib3/connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    842\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 844\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[1;32m    845\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    846\u001b[0m )\n\u001b[1;32m    847\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/Narsil/asr_dummy/resolve/main/mlk.flac (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x755f689aa610>: Failed to establish a new connection: [Errno 101] Network is unreachable'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 使用 Hugging Face Datasets 上的测试文件\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m preds \u001b[38;5;241m=\u001b[39m classifier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\u001b[39;00m\n\u001b[1;32m      5\u001b[0m preds\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/transformers/pipelines/audio_classification.py:136\u001b[0m, in \u001b[0;36mAudioClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    105\u001b[0m     inputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    107\u001b[0m ):\n\u001b[1;32m    108\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    Classify the sequence(s) given as inputs. See the [`AutomaticSpeechRecognitionPipeline`] documentation for more\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m    information.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m        - **score** (`float`) -- The corresponding probability.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/transformers/pipelines/base.py:1162\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1155\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1156\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[1;32m   1160\u001b[0m     )\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/transformers/pipelines/base.py:1168\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1168\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m   1169\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1170\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/transformers/pipelines/audio_classification.py:152\u001b[0m, in \u001b[0;36mAudioClassificationPipeline.preprocess\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;66;03m# We need to actually check for a real protocol, otherwise it's impossible to use a local file\u001b[39;00m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;66;03m# like http_huggingface_co.png\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(inputs)\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/data/minconda3/envs/llm/lib/python3.11/site-packages/requests/adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    516\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/Narsil/asr_dummy/resolve/main/mlk.flac (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x755f689aa610>: Failed to establish a new connection: [Errno 101] Network is unreachable'))"
     ]
    }
   ],
   "source": [
    "# 使用 Hugging Face Datasets 上的测试文件\n",
    "preds = classifier(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n",
    "\n",
    "#preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff9a964-7e4d-4187-9631-7a89a0545f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-25 23:10:09--  https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\n",
      "Resolving huggingface.co (huggingface.co)... 108.160.169.181, 2a03:2880:f10c:83:face:b00c:0:25de\n",
      "Connecting to huggingface.co (huggingface.co)|108.160.169.181|:443... "
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "655c614c-9eed-4ed2-bf49-63c66ecb1582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 Hugging Face Datasets 上的测试文件\n",
    "preds = classifier(\"./data/audio/mlk.flac\")\n",
    "#preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d76f6f39-6090-46dc-9428-1d097564804a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://hf-mirror.com/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(task=\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12f55bdd-bb0c-44bf-a745-f10ec856c8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'I-ORG', 'score': 0.9968, 'index': 1, 'word': 'Hu', 'start': 0, 'end': 2}\n",
      "{'entity': 'I-ORG', 'score': 0.9293, 'index': 2, 'word': '##gging', 'start': 2, 'end': 7}\n",
      "{'entity': 'I-ORG', 'score': 0.9763, 'index': 3, 'word': 'Face', 'start': 8, 'end': 12}\n",
      "{'entity': 'I-MISC', 'score': 0.9983, 'index': 6, 'word': 'French', 'start': 18, 'end': 24}\n",
      "{'entity': 'I-LOC', 'score': 0.999, 'index': 10, 'word': 'New', 'start': 42, 'end': 45}\n",
      "{'entity': 'I-LOC', 'score': 0.9987, 'index': 11, 'word': 'York', 'start': 46, 'end': 50}\n",
      "{'entity': 'I-LOC', 'score': 0.9992, 'index': 12, 'word': 'City', 'start': 51, 'end': 55}\n"
     ]
    }
   ],
   "source": [
    "preds = classifier(\"Hugging Face is a French company based in New York City.\")\n",
    "preds = [\n",
    "    {\n",
    "        \"entity\": pred[\"entity\"],\n",
    "        \"score\": round(pred[\"score\"], 4),\n",
    "        \"index\": pred[\"index\"],\n",
    "        \"word\": pred[\"word\"],\n",
    "        \"start\": pred[\"start\"],\n",
    "        \"end\": pred[\"end\"],\n",
    "    }\n",
    "    for pred in preds\n",
    "]\n",
    "print(*preds, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4513b7dd-ec84-4169-bdcc-88485026d3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://hf-mirror.com/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/data/minconda3/envs/llm/lib/python3.11/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'ORG',\n",
       "  'score': 0.9674639,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 0,\n",
       "  'end': 12},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.99828726,\n",
       "  'word': 'French',\n",
       "  'start': 18,\n",
       "  'end': 24},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.99896103,\n",
       "  'word': 'New York City',\n",
       "  'start': 42,\n",
       "  'end': 55}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(task=\"ner\", grouped_entities=True)\n",
    "classifier(\"Hugging Face is a French company based in New York City.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
